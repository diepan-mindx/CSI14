{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad76fa20",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c5be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "import os\n",
    "\n",
    "DATA_DIR = os.path.join(\n",
    "    path,\n",
    "    \"asl_alphabet_train\",\n",
    "    \"asl_alphabet_train\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee629ac",
   "metadata": {},
   "source": [
    "# Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f000a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4fbd8",
   "metadata": {},
   "source": [
    "# Chuan hoa du lieu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be59d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cắt tập train : validation + scale ảnh\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255, # chuyen doi gia tri [0, 255] ve [0, 1]\n",
    "    validation_split=0.2 # chia du lieu thanh 80% train va 20% validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training' # chi dinh lay tap train\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation' # chi dinh lay tap validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f257b5",
   "metadata": {},
   "source": [
    "# Khai bao mo hinh - fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79050737",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        Conv2D(128, (3,3), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'), # fully connected layer\n",
    "        Dropout(0.5),  # dropout de tranh overfitting\n",
    "        Dense(train_data.num_classes, activation='softmax') # 29 classes trong ASL alphabet\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17e595aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 217ms/step - accuracy: 0.6513 - loss: 1.1467 - val_accuracy: 0.6860 - val_loss: 1.0465\n",
      "Epoch 2/10\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 201ms/step - accuracy: 0.9392 - loss: 0.1859 - val_accuracy: 0.7367 - val_loss: 1.1271\n",
      "Epoch 3/10\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 200ms/step - accuracy: 0.9707 - loss: 0.0895 - val_accuracy: 0.7598 - val_loss: 1.0887\n",
      "Epoch 4/10\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 257ms/step - accuracy: 0.9828 - loss: 0.0537 - val_accuracy: 0.7618 - val_loss: 1.2040\n",
      "Epoch 5/10\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 227ms/step - accuracy: 0.9854 - loss: 0.0450 - val_accuracy: 0.7711 - val_loss: 1.0802\n",
      "Epoch 6/10\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 231ms/step - accuracy: 0.9879 - loss: 0.0393 - val_accuracy: 0.7295 - val_loss: 1.3172\n",
      "Epoch 7/10\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 259ms/step - accuracy: 0.9905 - loss: 0.0310 - val_accuracy: 0.7687 - val_loss: 1.3187\n",
      "Epoch 8/10\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 228ms/step - accuracy: 0.9924 - loss: 0.0240 - val_accuracy: 0.7941 - val_loss: 1.3161\n",
      "Epoch 9/10\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 216ms/step - accuracy: 0.9931 - loss: 0.0219 - val_accuracy: 0.7877 - val_loss: 1.1542\n",
      "Epoch 10/10\n",
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 220ms/step - accuracy: 0.9928 - loss: 0.0228 - val_accuracy: 0.7625 - val_loss: 1.2635\n"
     ]
    }
   ],
   "source": [
    "# huan luyen\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    validation_data=val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e167097f",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fbf0e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras._tf_keras.keras' has no attribute 'saving'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43msaving\u001b[49m.save_model(model, \u001b[33m'\u001b[39m\u001b[33mmy_model.keras\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# .h5: dung de load lai va du doan cho sau nay\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSaved model\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'keras._tf_keras.keras' has no attribute 'saving'"
     ]
    }
   ],
   "source": [
    "model.save('asl_alphabet_model.h5') # .h5: dung de load lai va du doan cho sau nay\n",
    "print('Saved model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49bfec",
   "metadata": {},
   "source": [
    "# Danh gia mo hinh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ea7044c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 117ms/step - accuracy: 0.7625 - loss: 1.2635\n",
      "Accuracy: 0.7625\n",
      "Loss: 1.2635\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(val_data)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd911f3e",
   "metadata": {},
   "source": [
    "# Du doan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c292905f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load mo hinh \n",
    "loaded_model = keras.models.load_model('asl_alphabet_model.h5')\n",
    "print('Loaded model from disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "487910ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n"
     ]
    }
   ],
   "source": [
    "# chuan bi label (chu cai)\n",
    "class_names = list(train_data.class_indices.keys())\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "954c8499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# load 1 hinh de xu ly\n",
    "img_dir = os.path.join(path, \"asl_alphabet_train\", \"asl_alphabet_train\", \"M\")  # duong dan den thu muc chua hinh\n",
    "# Get first image file\n",
    "img_file = os.listdir(img_dir)[0]\n",
    "img_path = os.path.join(img_dir, img_file) # duong dan den hinh can du doan\n",
    "# img_path = input(\"Enter the path of the image to predict: \")\n",
    "img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = img_array / 255.0  # scale anh\n",
    "img_array = np.expand_dims(img_array, axis=0)  # them kich thuoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3cd70234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "[[4.9077283e-18 4.4073430e-24 3.1672995e-35 7.8456207e-38 2.6034555e-17\n",
      "  9.9570848e-27 1.3169288e-16 2.8684500e-23 2.1881641e-21 2.8176604e-13\n",
      "  4.4430905e-19 1.0377214e-19 1.0000000e+00 4.1443796e-11 6.2759394e-23\n",
      "  4.3143451e-21 4.4553306e-25 2.3015677e-17 2.9727523e-13 1.4828903e-16\n",
      "  1.1010739e-13 2.2052127e-17 2.5302700e-22 1.8775224e-12 5.7355473e-19\n",
      "  1.3584987e-25 1.4305641e-18 4.0314793e-25 5.8764986e-24]]\n",
      "Predicted label: M\n"
     ]
    }
   ],
   "source": [
    "# du doan\n",
    "prediction = loaded_model.predict(img_array)\n",
    "predicted_index = np.argmax(prediction)\n",
    "predicted_label = class_names[predicted_index] # map index -> label\n",
    "\n",
    "# print % cac lop du doan\n",
    "print(prediction)\n",
    "print(f'Predicted label: {predicted_label}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
